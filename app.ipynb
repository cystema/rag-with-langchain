{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an LLM application using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai #part of Google’s Generative AI tools\n",
    "from IPython.display import display #displaying rich outputs in Jupyter Notebooks\n",
    "from IPython.display import Markdown #display text in Markdown format within Jupyter notebooks\n",
    "import textwrap #text manipulation, especially wrapping and formatting text output\n",
    "from langchain_community.document_loaders import PyPDFLoader  #load and parse content from PDF documents\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter #split the documents into smaller chunks for efficient processing\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings #integrates Google’s embedding models with LangChain. \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI #extension of LangChain that helps interact with Google’s generative AI chat models\n",
    "from langchain.vectorstores import Chroma #vector database designed for faster and more efficient similarity search.\n",
    "from langchain.chains import RetrievalQA #function is tailored to build question-answering systems using RAG. \n",
    "\n",
    "\n",
    "genai.configure(api_key='')\n",
    "model = genai.GenerativeModel('gemini-pro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Ask The Questions Using Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727249601.311462     114 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Generates new content or data from scratch:** Generative AI uses algorithms to create novel and unique text, images, audio, or code, without relying on existing data.\n",
      "* **Learns from large datasets:** It is trained on vast amounts of data to understand patterns and relationships, enabling it to generate content that is realistic, coherent, and knowledgeable.\n",
      "* **Offers creative and innovative solutions:** Generative AI empowers humans to explore new ideas, generate fresh perspectives, and enhance creative output by automating the creation of novel content.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "* **Generates new content or data from scratch:** Generative AI uses algorithms to create novel and unique text, images, audio, or code, without relying on existing data.\n",
       "* **Learns from large datasets:** It is trained on vast amounts of data to understand patterns and relationships, enabling it to generate content that is realistic, coherent, and knowledgeable.\n",
       "* **Offers creative and innovative solutions:** Generative AI empowers humans to explore new ideas, generate fresh perspectives, and enhance creative output by automating the creation of novel content."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate_content(\"Explain Generative AI in 3 Bullet Points\")\n",
    "print(response.text)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Chat With Gemini And Retrieve The Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts {\n",
      "  text: \"Hi! Proivide a recipe to make a margeritta pizza from scratch\"\n",
      "}\n",
      "role: \"user\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "parts {\n",
      "  text: \"**Ingredients:**\\n\\n**For the Dough:**\\n\\n* 2 1/2 cups (300g) all-purpose flour, plus more for dusting\\n* 1 teaspoon (5g) active dry yeast\\n* 1 teaspoon (5g) sugar\\n* 1 teaspoon (5g) kosher salt\\n* 1 cup (240ml) warm water (105-115°F / 40-46°C)\\n\\n**For the Pizza:**\\n\\n* 1/2 cup (120ml) tomato sauce or crushed tomatoes\\n* 1 ball (225g) fresh mozzarella cheese, sliced thinly\\n* 1/2 cup (15g) freshly grated Parmesan cheese\\n* 1/2 cup (20g) fresh basil leaves, torn into small pieces\\n* Olive oil, for greasing\\n\\n**Instructions:**\\n\\n**To Make the Dough:**\\n\\n1. In a large bowl, combine the flour, yeast, sugar, and salt.\\n2. Add the warm water and stir until the dough forms a sticky ball.\\n3. Turn the dough out onto a lightly floured surface and knead for 5-7 minutes until it becomes smooth and elastic.\\n4. Form the dough into a ball, place it in a lightly oiled bowl, and cover with plastic wrap.\\n5. Let the dough rise in a warm place for 1-2 hours, or until it has doubled in size.\\n\\n**To Make the Pizza:**\\n\\n1. Preheat oven to 500°F (260°C).\\n2. Grease a 12-inch (30cm) pizza pan or baking sheet with olive oil.\\n3. Punch down the risen dough and divide it into two equal parts.\\n4. Roll out one half of the dough into a 12-inch circle, and transfer it to the prepared pan.\\n5. Spread the tomato sauce evenly over the crust, leaving a 1-inch border.\\n6. Top with the mozzarella and Parmesan cheeses.\\n7. Sprinkle with the basil leaves.\\n8. Bake in the preheated oven for 10-12 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\\n9. Remove from the oven and let cool for a few minutes before slicing and serving.\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_tokens: 16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start chat and store it in the history\n",
    "hist = model.start_chat()\n",
    "\n",
    "#Get Response\n",
    "response = hist.send_message(\"Hi! Proivide a recipe to make a margeritta pizza from scratch\")\n",
    "\n",
    "#Get Markdown Output\n",
    "Markdown(response.text)\n",
    "\n",
    "#Get all items from history\n",
    "#Contains parts, and role objects\n",
    "#role can be user or model\n",
    "for item in hist.history:\n",
    "    print(item)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "#First object in item.parts\n",
    "item.parts[0].text\n",
    "\n",
    "# Count the number of tokens\n",
    "model.count_tokens(\"Now provide the location of the nearest supermarket where I can buy the ingredients from.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Experiment With The Temperature Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " For temperature 0.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting and decision trees. It builds an ensemble of decision trees, where each tree is trained on a weighted version of the training data. The weights are adjusted based on the errors made by the previous trees, ensuring that subsequent trees focus on correcting the mistakes of their predecessors.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Fraud Detection:** XGBoost can identify fraudulent transactions by analyzing patterns in financial data.\n",
       "* **Customer Churn Prediction:** It can predict the likelihood of customers leaving a service by considering factors such as usage history and demographics.\n",
       "* **Recommendation Systems:** XGBoost can recommend products or services to users based on their past preferences and interactions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the training data and a random subset of features. The final prediction is made by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Image Classification:** Random Forest can classify images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** It can be used for tasks such as text classification, sentiment analysis, and spam detection.\n",
       "* **Medical Diagnosis:** Random Forest can assist in diagnosing diseases by analyzing patient data, such as symptoms, medical history, and test results.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both XGBoost and Random Forest are highly accurate algorithms. However, XGBoost tends to perform slightly better on complex datasets.\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially for large datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees that make up the ensemble.\n",
       "* **Hyperparameter Tuning:** XGBoost requires more hyperparameter tuning than Random Forest, which can be time-consuming.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are both powerful machine learning algorithms with a wide range of applications. XGBoost is particularly well-suited for tasks that require high accuracy and speed, while Random Forest is more appropriate when interpretability is important. By understanding the strengths and weaknesses of each algorithm, data scientists can select the best tool for their specific problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " For temperature 0.25, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost**\n",
       "\n",
       "**Concept:** XGBoost (Extreme Gradient Boosting) is a machine learning algorithm that combines multiple decision trees into a single, highly accurate model. It uses a gradient boosting approach, where each tree is trained on the residuals of the previous tree, resulting in a cumulative improvement in predictive performance.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "\n",
       "* **Predicting Customer Churn:** XGBoost can be used to predict which customers are likely to churn (stop using a service). By identifying these customers, businesses can implement targeted retention strategies to minimize churn rates.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:** Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the data and makes predictions independently. The final prediction is determined by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "\n",
       "* **Image Classification:** Random Forest can be used to classify images into different categories. By training the algorithm on a large dataset of labeled images, it can learn to recognize patterns and make accurate predictions.\n",
       "\n",
       "**Comparison of XGBoost and Random Forest**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Model Type | Gradient Boosting | Ensemble of Decision Trees |\n",
       "| Regularization | Yes | Yes |\n",
       "| Feature Importance | Yes | Yes |\n",
       "| Computational Complexity | Higher | Lower |\n",
       "| Accuracy | Typically higher | Typically lower |\n",
       "| Interpretability | Lower | Higher |\n",
       "\n",
       "**When to Use XGBoost vs. Random Forest**\n",
       "\n",
       "* Use XGBoost when you need high accuracy and are willing to trade off interpretability for performance.\n",
       "* Use Random Forest when you need a more interpretable model and are willing to sacrifice some accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " For temperature 0.5, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is a scalable and efficient gradient boosting algorithm that combines the power of multiple decision trees to create a robust and accurate prediction model.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Gradient Boosting:** XGBoost uses a series of decision trees to iteratively improve the prediction accuracy. Each tree focuses on correcting the errors made by previous trees.\n",
       "* **Regularization:** XGBoost employs regularization techniques to prevent overfitting, such as L1 and L2 regularization.\n",
       "* **Parallelization:** XGBoost can be parallelized for faster training on large datasets.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Fraud Detection:** Identifying fraudulent transactions by analyzing historical data and patterns.\n",
       "* **Credit Risk Assessment:** Predicting the likelihood of loan default based on borrower characteristics and financial history.\n",
       "* **Stock Market Prediction:** Forecasting stock prices by considering market conditions, company fundamentals, and historical trends.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees to create a more accurate and robust model.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Bagging:** Random Forest generates multiple decision trees using different subsets of the data and features.\n",
       "* **Random Feature Selection:** Each tree is constructed using a random subset of features, reducing the impact of correlated variables.\n",
       "* **Majority Voting:** The final prediction is determined by the majority vote of the individual decision trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Image Classification:** Classifying images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** Identifying sentiment, extracting keyphrases, and performing text classification.\n",
       "* **Medical Diagnosis:** Predicting diseases or patient outcomes based on medical data, such as symptoms, test results, and medical history.\n",
       "\n",
       "**Comparison**\n",
       "\n",
       "Both XGBoost and Random Forest are powerful machine learning algorithms, but they have different strengths and weaknesses:\n",
       "\n",
       "* **Accuracy:** XGBoost generally has higher accuracy than Random Forest, especially for complex and large datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the contributions of individual decision trees.\n",
       "* **Computational Cost:** XGBoost can be more computationally expensive than Random Forest, especially for large datasets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " For temperature 0.75, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is a gradient boosting algorithm that combines multiple decision trees to make predictions. It is known for its speed, accuracy, and ability to handle large datasets.\n",
       "\n",
       "**Use Case:** Predicting customer churn. A telecom company can use XGBoost to predict which customers are likely to cancel their subscriptions. By identifying these customers, the company can take proactive measures to retain them.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the data and makes predictions independently. The final prediction is determined by combining the predictions of all the trees.\n",
       "\n",
       "**Use Case:** Detecting credit card fraud. A financial institution can use Random Forest to detect fraudulent transactions. The algorithm can learn from historical data to identify patterns that are indicative of fraud, such as unusual spending behavior or suspicious IP addresses.\n",
       "\n",
       "**Key Differences**\n",
       "\n",
       "* **Regularization:** XGBoost uses a more advanced regularization technique called \"tree pruning\" to prevent overfitting. Random Forest relies on bagging and feature randomization to reduce overfitting.\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially for large datasets.\n",
       "* **Hyperparameter Tuning:** XGBoost has more hyperparameters to tune, which can make it more difficult to optimize.\n",
       "* **Interpretability:** Random Forest is generally more interpretable than XGBoost, as it is easier to understand the individual decision trees and their contributions to the final prediction.\n",
       "\n",
       "**Advantages of XGBoost and Random Forest**\n",
       "\n",
       "* **High Accuracy:** Both algorithms can achieve high accuracy on a wide range of tasks.\n",
       "* **Robustness:** They are relatively insensitive to noise and outliers in the data.\n",
       "* **Scalability:** They can handle large datasets efficiently.\n",
       "* **Feature Importance:** They provide insights into the importance of different features in making predictions.\n",
       "\n",
       "**Disadvantages of XGBoost and Random Forest**\n",
       "\n",
       "* **Overfitting:** Both algorithms can overfit the data if not properly tuned.\n",
       "* **Interpretability:** XGBoost can be less interpretable than Random Forest, especially for complex models.\n",
       "* **Computation Time:** Hyperparameter tuning can be time-consuming, especially for XGBoost."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " For temperature 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost**\n",
       "\n",
       "XGBoost (Extreme Gradient Boosting) is a machine learning algorithm designed for extremely fast execution and high accuracy. It belongs to the family of gradient boosting algorithms that combine multiple weak learners (e.g., decision trees) into a stronger, more accurate model.\n",
       "\n",
       "**Concepts:**\n",
       "\n",
       "* **Tree Ensembles:** XGBoost uses a sequence of decision trees arranged hierarchically, boosting each tree's predictions based on the errors of previous trees.\n",
       "* **Regularization:** XGBoost includes regularization terms to prevent overfitting and improve model generalization.\n",
       "* **Parallelization:** XGBoost is parallelizable, allowing for efficient computation even on large datasets.\n",
       "\n",
       "**Use Cases:**\n",
       "\n",
       "* **Fraud Detection:** Detecting fraudulent transactions by identifying deviations from normal spending patterns.\n",
       "* **Customer Churn Prediction:** Predicting customers who are likely to cancel their subscriptions based on historical data and usage patterns.\n",
       "* **Image Classification:** Categorizing images into predefined classes, such as animals, vehicles, or objects.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble machine learning algorithm that combines multiple decision trees to improve stability and accuracy. It operates by selecting a random subset of features and data points for each tree, reducing correlation between trees and minimizing overfitting.\n",
       "\n",
       "**Concepts:**\n",
       "\n",
       "* **Decision Trees:** Random Forest builds a collection of decision trees using a random subsample of the training data.\n",
       "* **Voting:** Each tree independently makes predictions, and the final prediction is typically determined by a majority vote.\n",
       "* **Bagging:** Random Forest uses bagging (bootstrap aggregation), where multiple samples are randomly drawn from the training data to construct individual trees.\n",
       "\n",
       "**Use Cases:**\n",
       "\n",
       "* **Financial Forecasting:** Predicting financial performance metrics, such as stock prices or company revenues.\n",
       "* **Medical Diagnosis:** Assisting medical professionals in diagnosing diseases by analyzing patient data and symptoms.\n",
       "* **Object Detection:** Identifying and localizing objects in images or videos.\n",
       "\n",
       "**Comparison**\n",
       "\n",
       "XGBoost and Random Forest offer similar capabilities but differ in certain aspects:\n",
       "\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially on large datasets.\n",
       "* **Regularization:** XGBoost has built-in regularization features, while Random Forest requires manual hyperparameter tuning to prevent overfitting.\n",
       "* **Parallelization:** XGBoost can be parallelized across multiple cores, allowing for faster training.\n",
       "* **Feature Selection:** Random Forest supports feature selection, while XGBoost typically requires an external selection process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response(prompt, config):\n",
    "    response = model.generate_content(contents=prompt, generation_config=config )\n",
    "    return response\n",
    "\n",
    "for temp in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    config = genai.types.GenerationConfig(temperature=temp)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", config)\n",
    "\n",
    "    print(f\"\\n\\n For temperature {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Experiment With Maximum Output Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For max output token value 1, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For max output token value 50, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is an advanced ensemble machine learning algorithm that combines multiple decision trees. It uses a technique called gradient boosting, which sequentially adds decision trees to a model to improve its accuracy.\n",
       "\n",
       "**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For max output token value 100, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost**\n",
       "\n",
       "XGBoost is an ensemble machine learning algorithm that uses gradient boosting to iteratively build a series of decision trees. Each subsequent tree is trained to predict the residual errors of the previous tree. XGBoost is known for its speed, accuracy, and ability to handle large datasets.\n",
       "\n",
       "**Real-life use cases:**\n",
       "\n",
       "* **Credit risk assessment:** Predicting the likelihood of a borrower defaulting on a loan.\n",
       "* **Fraud detection:** Identifying fraudulent transactions in financial"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For max output token value 150, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Concept of XGBoost and Random Forest**\n",
       "\n",
       "**XGBoost (eXtreme Gradient Boosting):**\n",
       "* An ensemble learning algorithm that combines multiple decision trees to create a powerful predictive model.\n",
       "* Uses gradient boosting, where each tree learns from the errors of previous trees.\n",
       "* Highly efficient and effective for large datasets.\n",
       "\n",
       "**Random Forest:**\n",
       "* An ensemble learning algorithm that combines multiple decision trees.\n",
       "* Each tree in the forest is trained on a different subset of the training data and a different subset of features.\n",
       "* The final prediction is made by majority vote or averaging the predictions of individual trees.\n",
       "\n",
       "**Real-Life Use Cases**\n",
       "\n",
       "**XGBoost:**\n",
       "\n",
       "* **Retail Recommendation:** Predicting product recommendations"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For max output token value 200, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:** XGBoost is an ensemble machine learning algorithm that combines multiple weak learners (decision trees) into a powerful, predictive model. It uses a gradient boosting framework to optimize the objective function iteratively, minimizing the error at each step.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Fraud Detection:** Identifying fraudulent transactions in financial data by analyzing customer behavior.\n",
       "* **Healthcare Risk Prediction:** Predicting the risk of developing certain diseases based on medical history and lifestyle factors.\n",
       "* **Natural Language Processing:** Enhancing text classification and sentiment analysis by capturing non-linear relationships in text.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:** Random Forest is an ensemble learning method that constructs multiple randomized decision trees. Each tree is trained on a subset of the training data, and their predictions are combined to make a final decision.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Image Classification:** Classifying images into different categories, such as people,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, generation_config=generation_config)\n",
    "    return response\n",
    "for max_out_tok in [1, 50, 100, 150, 200]:\n",
    "    config = genai.types.GenerationConfig(max_output_tokens=max_out_tok)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor max output token value {max_out_tok}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Experiment With the top_k Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 1, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost** (Extreme Gradient Boosting) and **Random Forest** are two popular ensemble machine learning algorithms used for supervised learning tasks, particularly for classification and regression problems.\n",
       "\n",
       "**XGBoost**\n",
       "\n",
       "* **Concept:** XGBoost is a gradient boosting algorithm that builds a series of decision trees. It uses a regularized objective function to prevent overfitting and improves the accuracy by optimizing each tree sequentially.\n",
       "* **Key Features:**\n",
       "    * Regularization: XGBoost uses L1 and L2 regularization to penalize complex models and reduce overfitting.\n",
       "    * Tree Pruning: It uses a technique called \"pruning\" to remove less important branches from the decision trees, improving model efficiency.\n",
       "    * Parallel Processing: XGBoost can be parallelized to train models on large datasets efficiently.\n",
       "* **Use Cases:**\n",
       "    * Credit Scoring: Predicting the probability of default on a loan.\n",
       "    * Customer Churn: Identifying customers at risk of leaving a service.\n",
       "    * Stock Price Prediction: Forecasting stock prices based on historical data.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "* **Concept:** Random Forest is an ensemble algorithm that combines multiple decision trees. It constructs each tree using a different subset of data and features, reducing the variance and improving the overall accuracy.\n",
       "* **Key Features:**\n",
       "    * Feature Randomization: Random Forest randomly selects a subset of features for each tree, increasing diversity among the trees.\n",
       "    * Bootstrap Sampling: Each tree is trained on a bootstrapped sample of the original dataset, further reducing overfitting.\n",
       "    * Majority Voting: For classification tasks, the output of Random Forest is determined by the majority vote of the individual trees.\n",
       "* **Use Cases:**\n",
       "    * Spam Detection: Filtering out spam emails from a dataset.\n",
       "    * Image Recognition: Classifying images into different categories.\n",
       "    * Natural Language Processing: Identifying topics or sentiments in text data.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both algorithms are known for their high accuracy, with XGBoost being slightly more accurate in general.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to visualize and understand the decision-making process of individual trees.\n",
       "* **Hyperparameter Tuning:** XGBoost has more hyperparameters to tune, which can be both an advantage and a disadvantage.\n",
       "* **Computational Complexity:** XGBoost is computationally more expensive than Random Forest, especially for larger datasets.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are powerful ensemble algorithms that offer high accuracy and robustness for supervised learning tasks. The choice between the two depends on factors such as the size of the dataset, the level of interpretability required, and the computational resources available."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 4, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:** XGBoost is an ensemble machine learning algorithm based on gradient boosting. It trains multiple decision trees sequentially, where each subsequent tree focuses on correcting the errors of the previous ones. XGBoost optimizes the trees' structure and weights to minimize the overall loss function.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* Predicting customer churn: Identifying customers at risk of leaving a service or subscription.\n",
       "* Detecting fraudulent transactions: Flagging suspicious credit card or financial transactions.\n",
       "* Personalizing recommendations: Tailoring product or content recommendations to individual users.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:** Random forest is an ensemble machine learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the data and a random selection of features. The final prediction is made by combining the predictions of all the trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* Classifying medical images: Identifying different types of diseases or tissues in medical scans.\n",
       "* Predicting stock prices: Forecasting the future value of stocks based on historical data and market trends.\n",
       "* Spam detection: Distinguishing between legitimate and spam emails.\n",
       "\n",
       "**Comparison between XGBoost and Random Forest**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Regularization | Supports regularization techniques like L1 and L2 | Supports regularization techniques like L1 and L2 |\n",
       "| Hyperparameter Tuning | Requires careful hyperparameter tuning | Requires less hyperparameter tuning |\n",
       "| Computational Cost | More computationally intensive | Less computationally intensive |\n",
       "| Performance | Generally higher accuracy and predictive power | Generally lower accuracy and predictive power |\n",
       "| Interpretability | Less interpretable compared to Random Forest | More interpretable compared to XGBoost |\n",
       "\n",
       "**Choice of Algorithm:**\n",
       "\n",
       "* **XGBoost** is preferred for high-dimensional data, complex problems, and when interpretability is not a major concern.\n",
       "* **Random Forest** is preferred for problems where interpretability is important, such as medical diagnosis or patient risk assessment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 16, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## XGBoost\n",
       "\n",
       "XGBoost (Extreme Gradient Boosting) is an ensemble machine learning algorithm that combines multiple decision trees to create a more powerful and accurate model. It is a popular technique for solving a wide range of classification and regression problems.\n",
       "\n",
       "**Key Concepts of XGBoost:**\n",
       "\n",
       "* **Boosting:** A technique where multiple weak learners (decision trees) are combined to create a strong learner.\n",
       "* **Gradient Boosting:** A specific type of boosting that uses the gradient of the loss function to update the subsequent trees.\n",
       "* **Regularization:** A technique that prevents overfitting by penalizing complex models.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Predicting Credit Risk:** XGBoost can be used to predict the likelihood of a borrower defaulting on a loan.\n",
       "* **Fraud Detection:** It can help identify fraudulent transactions by analyzing patterns in historical data.\n",
       "* **Sales Forecasting:** XGBoost can be used to predict future sales based on historical data and various factors.\n",
       "\n",
       "## Random Forest\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the data, and the final prediction is made by combining the predictions of all the trees.\n",
       "\n",
       "**Key Concepts of Random Forest:**\n",
       "\n",
       "* **Bagging:** A technique where multiple decision trees are trained on different subsets of the data to reduce variance.\n",
       "* **Feature Subsetting:** Random Forest randomly selects a subset of features to use in each tree, improving model generalization.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Image Classification:** Random Forest can be used to classify images into different categories, such as animals, objects, and scenes.\n",
       "* **Spamb Filtering:** It can help identify and filter spam emails based on patterns in the content and sender information.\n",
       "* **Medical Diagnosis:** Random Forest can assist in diagnosing medical conditions by analyzing patient data and symptoms.\n",
       "\n",
       "## Comparison of XGBoost and Random Forest\n",
       "\n",
       "| Feature       | XGBoost       | Random Forest         |\n",
       "|---|---|---|\n",
       "| Tree Structure    | Boosting            | Bagging                 |\n",
       "| Feature Selection | Regularization         | Feature Subsetting      |\n",
       "| Computational Cost | Usually higher         | Usually lower          |\n",
       "| Interpretability  | Lower          | Higher                 |\n",
       "| Accuracy     | Generally higher         | Generally lower          |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 32, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost (eXtreme Gradient Boosting) is a machine learning algorithm that combines multiple decision trees to create a more accurate and robust model. It uses a technique called gradient boosting, where each tree is built to correct the errors of the previous trees by predicting the residual errors from the previous model.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "* **Predicting Credit Risk:** XGBoost can help banks assess the risk of potential borrowers by analyzing their financial data, credit history, and spending patterns. The model can predict the probability of loan default, enabling banks to make informed decisions about loan approvals and interest rates.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a subset of the training data, and the final prediction is made by combining the outputs of all the trees. The randomness in the algorithm helps prevent overfitting and improves generalization.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "* **Image Classification:** Random Forest can be used to classify images into different categories, such as animals, vehicles, or landscapes. The algorithm analyzes the features of the image, such as color, texture, and shape, to determine its probable category.\n",
       "\n",
       "**Comparison**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Model Type | Gradient Boosting | Ensemble of Decision Trees |\n",
       "| Regularization | Built-in | Randomness from Subsampling |\n",
       "| Hyperparameter Tuning | More complex | Easier |\n",
       "| Computational Cost | Higher | Lower |\n",
       "| Performance | Generally higher accuracy | Similar or lower accuracy |\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Both XGBoost and Random Forest are powerful machine learning algorithms with wide applications. XGBoost is known for its high accuracy and robustness, while Random Forest is simpler to implement and tune. The choice between the two algorithms depends on the specific task and the desired level of accuracy and computational efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 40, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is a supervised machine learning algorithm that combines multiple decision trees to make predictions. It is an ensemble method, meaning it combines the outputs of several weak learners to create a stronger learner.\n",
       "\n",
       "**Concepts:**\n",
       "\n",
       "* **Gradient Boosting:** XGBoost builds decision trees sequentially, using the gradient of the loss function to determine the optimal split at each tree node.\n",
       "* **Regularization:** XGBoost employs regularization techniques to prevent overfitting, such as L1 and L2 regularization.\n",
       "* **Tree Pruning:** XGBoost uses a technique called \"tree pruning\" to reduce the size and complexity of the trees, improving performance and interpretability.\n",
       "\n",
       "**Real-Life Use Case:** Predicting Customer Churn\n",
       "\n",
       "* A telecom company wants to predict which customers are likely to churn (cancel their service).\n",
       "* XGBoost can be used to build a model that combines information about customers' usage history, demographics, and account details.\n",
       "* The model can be used to identify high-risk customers and implement targeted retention strategies.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble method that combines multiple decision trees into a single model. It operates by randomly selecting a subset of features and data points to train each individual tree.\n",
       "\n",
       "**Concepts:**\n",
       "\n",
       "* **Bootstrapping:** Random Forest uses bootstrapping, where each tree is trained on a different subset of the data, increasing the diversity of the ensemble.\n",
       "* **Feature Bagging:** Random Forest performs feature bagging, where each tree is trained on a different subset of features, reducing the impact of correlated features.\n",
       "* **Aggregation:** The final prediction is made by combining the predictions of the individual trees, usually through majority voting or averaging.\n",
       "\n",
       "**Real-Life Use Case:** Image Classification\n",
       "\n",
       "* An online photo-sharing platform wants to automatically classify images into different categories.\n",
       "* Random Forest can be used to build a model that combines information about pixel values, textures, and shapes.\n",
       "* The model can be used to classify images into various categories, such as animals, landscapes, or portraits.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* Both XGBoost and Random Forest are ensemble methods that combine multiple decision trees.\n",
       "* XGBoost is typically more accurate and efficient than Random Forest, especially for larger datasets.\n",
       "* Random Forest is easier to interpret and implement due to its simpler algorithm.\n",
       "* The choice between XGBoost and Random Forest depends on the specific problem and the available resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for k in [1, 4, 16, 32, 40]:\n",
    "    config = genai.types.GenerationConfig(top_k=k)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor top k value {k}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Experiment With the top_p Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting and decision trees. It builds an ensemble of decision trees, where each tree is trained on a weighted version of the training data. The weights are adjusted based on the errors made by the previous trees, ensuring that subsequent trees focus on correcting the mistakes of their predecessors.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Fraud Detection:** XGBoost can identify fraudulent transactions by analyzing patterns in financial data.\n",
       "* **Customer Churn Prediction:** It can predict the likelihood of customers leaving a service by considering factors such as usage history and demographics.\n",
       "* **Recommendation Systems:** XGBoost can recommend products or services to users based on their past preferences and interactions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the training data and a random subset of features. The final prediction is made by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Image Classification:** Random Forest can classify images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** It can be used for tasks such as text classification, sentiment analysis, and spam detection.\n",
       "* **Medical Diagnosis:** Random Forest can assist in diagnosing diseases by analyzing patient data, such as symptoms, medical history, and test results.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both XGBoost and Random Forest are highly accurate algorithms. However, XGBoost tends to perform slightly better on complex datasets.\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially for large datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees that make up the ensemble.\n",
       "* **Hyperparameter Tuning:** XGBoost requires more hyperparameter tuning than Random Forest, which can be time-consuming.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are both powerful machine learning algorithms with a wide range of applications. XGBoost is particularly well-suited for tasks that require high accuracy and speed, while Random Forest is more appropriate when interpretability is important. By understanding the strengths and weaknesses of each algorithm, data scientists can select the best tool for their specific problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting and decision trees. It builds an ensemble of decision trees, where each tree is trained on a weighted version of the training data. The weights are adjusted based on the errors made by the previous trees, ensuring that subsequent trees focus on correcting the mistakes of their predecessors.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Fraud Detection:** XGBoost can identify fraudulent transactions by analyzing patterns in financial data.\n",
       "* **Customer Churn Prediction:** It can predict the likelihood of customers leaving a service by considering factors such as usage history and demographics.\n",
       "* **Recommendation Systems:** XGBoost can recommend products or services to users based on their past preferences and interactions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the training data and a random subset of features. The final prediction is made by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Image Classification:** Random Forest can classify images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** It can be used for tasks such as text classification, sentiment analysis, and spam detection.\n",
       "* **Medical Diagnosis:** Random Forest can assist in diagnosing diseases by analyzing patient data, such as symptoms, medical history, and test results.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both XGBoost and Random Forest are highly accurate algorithms. However, XGBoost tends to perform slightly better on complex datasets.\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially for large datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees that make up the ensemble.\n",
       "* **Hyperparameter Tuning:** XGBoost requires more hyperparameter tuning than Random Forest, which can be time-consuming.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are both powerful machine learning algorithms with a wide range of applications. XGBoost is particularly well-suited for tasks that require high accuracy and speed, while Random Forest is more appropriate when interpretability is important. The choice between the two algorithms depends on the specific requirements of the problem at hand."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is a machine learning algorithm that combines the power of gradient boosting with decision trees. It is known for its high accuracy and efficiency, making it a popular choice for a wide range of applications.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Gradient Boosting:** XGBoost uses gradient boosting, which involves iteratively building decision trees to minimize the loss function.\n",
       "* **Regularization:** XGBoost includes regularization terms to prevent overfitting and improve generalization.\n",
       "* **Tree Pruning:** XGBoost employs tree pruning techniques to reduce the complexity of the model and enhance its interpretability.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Fraud Detection:** XGBoost can identify fraudulent transactions by analyzing patterns in financial data.\n",
       "* **Customer Churn Prediction:** It can predict the likelihood of customers leaving a service or product based on their behavior and demographics.\n",
       "* **Medical Diagnosis:** XGBoost assists in diagnosing diseases by analyzing medical images and patient records.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions. It is known for its robustness and ability to handle large datasets.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Ensemble Learning:** Random Forest creates a collection of decision trees, each trained on a different subset of the data.\n",
       "* **Bagging:** It uses bagging, where each tree is trained on a random sample of the data with replacement.\n",
       "* **Feature Randomization:** Random Forest randomly selects a subset of features for each tree, reducing the risk of overfitting.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Image Classification:** Random Forest can classify images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** It can analyze text data for tasks like sentiment analysis and spam detection.\n",
       "* **Financial Forecasting:** Random Forest can predict stock prices or economic indicators based on historical data.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Accuracy | Higher | Comparable |\n",
       "| Efficiency | Lower | Higher |\n",
       "| Regularization | Yes | No |\n",
       "| Tree Pruning | Yes | No |\n",
       "| Interpretability | Lower | Higher |\n",
       "| Ensemble Method | Gradient Boosting | Bagging |\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are powerful machine learning algorithms with distinct strengths and weaknesses. XGBoost excels in tasks requiring high accuracy and efficiency, while Random Forest is more suitable for applications where interpretability and robustness are important. By understanding the concepts and use cases of these algorithms, practitioners can select the most appropriate tool for their specific problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost**\n",
       "\n",
       "XGBoost (eXtreme Gradient Boosting) is a powerful ensemble machine learning algorithm that combines the strengths of gradient boosting and regularization. It is widely used in a variety of machine learning tasks, including classification, regression, and ranking.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Gradient Boosting:** XGBoost builds an ensemble of decision trees, where each subsequent tree is trained to correct the errors of the previous trees.\n",
       "* **Regularization:** XGBoost employs various regularization techniques, such as L1 and L2 regularization, to prevent overfitting and improve generalization performance.\n",
       "* **Scalability:** XGBoost is highly scalable and can handle large datasets with millions of rows and thousands of features.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Predicting Customer Churn:** XGBoost can be used to identify customers at risk of churning and take proactive measures to retain them.\n",
       "* **Fraud Detection:** XGBoost is effective in detecting fraudulent transactions in financial datasets.\n",
       "* **Credit Scoring:** XGBoost models can help banks and other lenders assess the creditworthiness of borrowers.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that consists of a multitude of decision trees. It operates on the principle of bootstrap aggregation, where multiple decision trees are trained on different subsets of the data.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Bootstrap Aggregation:** Random Forest trains multiple decision trees on random subsets of the data, with replacement.\n",
       "* **Majority Voting:** Predictions are made by combining the outputs of all the individual decision trees, typically through majority voting or averaging.\n",
       "* **Feature Randomness:** Random Forest introduces randomness in feature selection, where a subset of features is randomly selected for each decision tree.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Image Classification:** Random Forest is widely used in image classification tasks, such as object detection and scene recognition.\n",
       "* **Text Classification:** Random Forest can be used to classify text documents into different categories, such as spam filtering or sentiment analysis.\n",
       "* **Medical Diagnosis:** Random Forest models can assist medical professionals in diagnosing diseases and predicting patient outcomes.\n",
       "\n",
       "**Comparison**\n",
       "\n",
       "Both XGBoost and Random Forest are powerful ensemble learning algorithms, but they differ in several aspects:\n",
       "\n",
       "* **Complexity:** XGBoost is generally more complex and requires more hyperparameter tuning compared to Random Forest.\n",
       "* **Accuracy:** XGBoost often achieves higher accuracy than Random Forest, especially on complex datasets.\n",
       "* **Interpretability:** Random Forest is generally more interpretable than XGBoost, as it is easier to understand the decision-making process of individual decision trees.\n",
       "* **Scalability:** XGBoost is more scalable than Random Forest and can handle larger datasets more efficiently."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines multiple weak models (decision trees) into a single strong model. It utilizes gradient boosting, where each new tree iteratively corrects the errors of the previous trees by focusing on data points that were misclassified.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "**Fraud Detection:** XGBoost is effective in identifying fraudulent transactions in financial datasets. It analyzes historical transaction patterns to detect anomalies and suspicious activities.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning method that creates multiple decision trees during training. Each tree is trained on a different subset of the data and uses a different feature subset for splitting at each node. The final prediction is made by combining the predictions from all individual trees.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "**Object Recognition:** Random Forest is commonly used in computer vision for object recognition. It trains multiple trees on different patches or regions of an image to improve accuracy and robustness.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both XGBoost and Random Forest can achieve high accuracy, but XGBoost is generally considered more accurate, especially for complex and structured datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost as the individual decision trees can be analyzed to understand the model's decision-making process.\n",
       "* **Computational Complexity:** XGBoost is more computationally intensive and requires longer training times than Random Forest.\n",
       "* **Tuning:** XGBoost has more hyperparameters to tune compared to Random Forest, which can be challenging but can lead to better performance.\n",
       "\n",
       "**Additional Use Cases:**\n",
       "\n",
       "**XGBoost:**\n",
       "* Sentiment Analysis\n",
       "* Click-Through Rate Prediction\n",
       "* Credit Risk Assessment\n",
       "\n",
       "**Random Forest:**\n",
       "* Image Classification\n",
       "* Medical Diagnosis\n",
       "* Time Series Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for p in [0, 0.2, 0.4, 0.8, 1]:\n",
    "    config = genai.types.GenerationConfig(top_p=p)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor top p value {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Experiment With the candidate_count Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting and decision trees. It sequentially builds decision trees, where each tree focuses on correcting the errors made by previous trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* Fraud Detection: Identifying fraudulent transactions by learning from historical data.\n",
       "* Click-Through Rate Prediction: Predicting the probability of a user clicking on an ad based on website features.\n",
       "* Credit Risk Assessment: Evaluating the risk of default for loan applicants.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that constructs multiple decision trees during training. Each tree randomly selects a subset of features and samples, creating a diverse set of trees. Predictions are made by combining the predictions of all individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* Image Classification: Classifying images into different categories, such as animals, objects, or scenes.\n",
       "* Natural Language Processing: Understanding and extracting meaning from text, including sentiment analysis and topic modeling.\n",
       "* Medical Diagnosis: Assisting doctors in diagnosing diseases by analyzing patient data.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Algorithm Type | Gradient Boosting | Ensemble Learning |\n",
       "| Base Learner | Decision Trees | Decision Trees |\n",
       "| Feature Selection | Weighted by importance | Random selection |\n",
       "| Model Complexity | Higher | Lower |\n",
       "| Training Time | Longer | Shorter |\n",
       "| Regularization | Regularized by tree depth | Regularized by number of trees |\n",
       "| Performance | Usually higher | Can vary depending on the dataset |\n",
       "\n",
       "**Choice of Algorithm:**\n",
       "\n",
       "* For highly complex tasks and when interpretability is less important, **XGBoost** is often preferred due to its superior performance.\n",
       "* For tasks where interpretability and computational efficiency are crucial, **Random Forest** may be a better choice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = genai.types.GenerationConfig(candidate_count=1)\n",
    "result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "Markdown(result.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Introduction to Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Load the PDF and Extract the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 700\n",
    "CHUNK_OVERLAP = 100\n",
    "pdf_path = \"https://www.analytixlabs.co.in/assets/pdfs/Data_Engineering%20&_Other_Job_Roles-AnalytixLabs.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_path)\n",
    "split_pdf_document = pdf_loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in split_pdf_document)\n",
    "texts = text_splitter.split_text(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Create the Gemini Model and Create the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=\"\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Chroma.from_texts(texts, embeddings)\n",
    "retriever = vector_index.as_retriever(search_kwargs={\"k\" : 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Create the RAG Chain and Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(gemini_model, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: * **Data integration tools:** These tools help data engineers connect to and extract data from various data sources, such as databases, spreadsheets, and cloud storage. Examples include Informatica PowerCenter, Talend Data Integration, and Microsoft SQL Server Integration Services (SSIS).\n",
      "* **Data transformation tools:** These tools help data engineers clean, transform, and prepare data for analysis and modeling. Examples include Apache Spark, Apache Hadoop, and Flink.\n",
      "* **Data warehousing tools:** These tools help data engineers create and manage data warehouses, which are central repositories of data for analysis and reporting. Examples include Amazon Redshift, Google BigQuery, and Microsoft Azure Synapse Analytics.\n",
      "* **Data visualization tools:** These tools help data engineers create visual representations of data, such as charts, graphs, and dashboards. Examples include Tableau, Power BI, and Google Data Studio.\n",
      "* **Data modeling tools:** These tools help data engineers create and manage data models, which are logical representations of data. Examples include ERwin Data Modeler, Microsoft Visio, and Oracle SQL Developer Data Modeler.\n",
      "* **Data governance tools:** These tools help data engineers manage and enforce data governance policies, which are rules and procedures for ensuring the accuracy, consistency, and security of data. Examples include Collibra Data Governance Center, Informatica Data Governance, and SAP Data Governance.\n",
      "* **Big data tools:** These tools help data engineers work with large datasets that are too large to be processed by traditional data processing tools. Examples include Apache Hadoop, Apache Spark, and Apache Flink.\n",
      "* **Machine learning tools:** These tools help data engineers build and train machine learning models, which can be used to predict future events or identify patterns in data. Examples include TensorFlow, PyTorch, and scikit-learn.\n"
     ]
    }
   ],
   "source": [
    "# Example usage \n",
    "question = \"Which tools do Data Engineers primarily work with?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(\"Answer:\", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
